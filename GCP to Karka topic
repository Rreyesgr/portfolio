pip install google-cloud-bigquery kafka-python

from google.cloud import bigquery
import json

from kafka import KafkaProducer

# Initialize BigQuery client
client = bigquery.Client.from_service_account_json('/path_to_your_service_account_key.json')

kafka_user = os.environ.get('KAFKA_USER')
kafka_pass = os.environ.get('KAFKA_PASS')

# Specify your BigQuery dataset and table
dataset_name = 'your_dataset_name'
table_name = 'your_table_name'

# Extract data from BigQuery table
table_id = f"{client.project}.{dataset_name}.{table_name}"
table = client.get_table(table_id)
rows = client.list_rows(table)
data = [dict(row) for row in rows]

# Save data to a JSON file
json_file_path = "/path_to_save_json_file/data.json"
with open(json_file_path, 'w') as f:
    json.dump(data, f)


# Initialize Kafka producer
producer = KafkaProducer(bootstrap_servers='your_kafka_bootstrap_servers',
                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))

topic_name = 'your_kafka_topic_name'

# Read the saved JSON file
with open(json_file_path, 'r') as f:
    json_data = json.load(f)

# Send data to Kafka topic
producer.send(topic_name, json_data)

producer.flush()  # Ensure all messages have been sent
producer.close()
